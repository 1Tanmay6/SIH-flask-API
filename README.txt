run 1: pip3 install -r requirements.txt
run 2: main.py

NOTE: LLAMA model is not in the github repo so the '/answer' route will not work if you want to make it work download the 'llama-2-7b-chat.ggmlv3.q8_0.bin' from onedrive and paste it in the folder same as main_flask.py
